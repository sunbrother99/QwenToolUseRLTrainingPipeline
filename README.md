# QwenToolUseRLTrainingPipeline
æˆ‘ä»¬æ¥æ„å»ºä¸€ä¸ªä¼ä¸šå†…éƒ¨ **æ¨¡ç³Šæé—® â†’ å¤šå·¥å…·è°ƒç”¨ â†’ RL å¾®è°ƒ** çš„å®Œæ•´é“¾æ¡ï¼ŒåŒ…æ‹¬ï¼š

- âœ… æ•°æ®æ ¼å¼è®¾è®¡ï¼ˆHuggingFace é£æ ¼ï¼‰
- âœ… SFT è®­ç»ƒè„šæœ¬ï¼ˆ`train_sft.py`ï¼‰
- âœ… Reward Model è®­ç»ƒè„šæœ¬ï¼ˆ`train_rm.py`ï¼‰
- âœ… PPO å¼ºåŒ–å­¦ä¹ è„šæœ¬ï¼ˆ`train_ppo.py`ï¼‰
- âœ… è¯„ä¼°æŒ‡æ ‡ï¼šTool-level + Chain-level å‡†ç¡®ç‡
- âœ… ç¤ºä¾‹é…ç½®æ–‡ä»¶ï¼ˆ`config.yaml`ï¼‰

---

## ğŸ“ é¡¹ç›®ç›®å½•ç»“æ„å»ºè®®

```
tool_use_rl_project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sft_data.jsonl
â”‚   â”œâ”€â”€ rm_data.jsonl
â”‚   â”œâ”€â”€ ppo_data.jsonl
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config_sft.yaml
â”‚   â”œâ”€â”€ config_rm.yaml
â”‚   â”œâ”€â”€ config_ppo.yaml
â”œâ”€â”€ train_sft.py
â”œâ”€â”€ train_rm.py
â”œâ”€â”€ train_ppo.py
â”œâ”€â”€ evaluate.py
â””â”€â”€ utils/
    â”œâ”€â”€ tool_schema.py  # å·¥å…·å®šä¹‰ + schema éªŒè¯
    â”œâ”€â”€ reward_metrics.py  # tool-level & chain-level è¯„ä¼°å‡½æ•°
```

---

## ğŸ“„ 1. æ•°æ®æ ¼å¼è¯´æ˜ï¼ˆHuggingFace SFT æ ¼å¼ï¼‰

### ğŸ”¹ SFT æ•°æ®ï¼ˆ`sft_data.jsonl`ï¼‰

```json
{
  "instruction": "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©åŒ—äº¬çš„å¤©æ°”",
  "input": "",
  "output": "{\"tool_call\": {\"tool_name\": \"queryWeather\", \"parameters\": {\"location\": \"åŒ—äº¬\", \"date\": \"2025-04-24\"}}}"
}
```

---

### ğŸ”¹ Reward Model æ•°æ®ï¼ˆ`rm_data.jsonl`ï¼‰

```json
{
  "prompt": "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ˜å¤©åŒ—äº¬çš„å¤©æ°”",
  "chosen": "queryWeather(location=åŒ—äº¬, date=2025-04-24)",
  "rejected": "queryWeather(location=ä¸Šæµ·, date=2025-04-24)"
}
```

---

### ğŸ”¹ PPO æ•°æ®ï¼ˆ`ppo_data.jsonl`ï¼‰

```json
{
  "prompt": "æŸ¥ä¸‹ç°åœ¨æ­å·é‚£è¾¹çš„æ°”æ¸©æ˜¯å¤šå°‘",
  "response": "queryWeather(location=æ­å·, date=now)"
}
```

---

## ğŸ§  2. SFT è®­ç»ƒè„šæœ¬ï¼ˆ`train_sft.py`ï¼‰

åŸºäº [TRL (Transformers Reinforcement Learning)](https://github.com/huggingface/trl)ï¼š

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset

model_name = "qwen/Qwen-7B-Chat"
dataset = load_dataset("json", data_files="data/sft_data.jsonl")

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset["train"],
    args=TrainingArguments(
        output_dir="output/sft",
        per_device_train_batch_size=2,
        num_train_epochs=3,
        learning_rate=5e-5,
    )
)
trainer.train()
```

---

## ğŸ¯ 3. Reward Model è®­ç»ƒè„šæœ¬ï¼ˆ`train_rm.py`ï¼‰

```python
from trl import RewardTrainer
from transformers import AutoModelForSequenceClassification
from datasets import load_dataset

model = AutoModelForSequenceClassification.from_pretrained("qwen/Qwen-7B", num_labels=1)
dataset = load_dataset("json", data_files="data/rm_data.jsonl")

trainer = RewardTrainer(
    model=model,
    train_dataset=dataset["train"],
    tokenizer=AutoTokenizer.from_pretrained("qwen/Qwen-7B"),
    args=TrainingArguments(output_dir="output/rm", num_train_epochs=3)
)
trainer.train()
```

---

## ğŸ’¥ 4. PPO å¼ºåŒ–å­¦ä¹ è„šæœ¬ï¼ˆ`train_ppo.py`ï¼‰

```python
from trl import PPOTrainer, PPOConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

ppo_config = PPOConfig(
    model_name="qwen/Qwen-7B-Chat",
    reward_tokenizer="output/rm",  # RM è·¯å¾„
    log_with=None,
)

ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=AutoModelForCausalLM.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True),
    tokenizer=AutoTokenizer.from_pretrained("qwen/Qwen-7B-Chat", trust_remote_code=True),
    dataset=load_dataset("json", data_files="data/ppo_data.jsonl")["train"]
)

ppo_trainer.train()
```

---

## ğŸ“Š 5. è¯„ä¼°è„šæœ¬ï¼ˆ`evaluate.py`ï¼‰

```python
from utils.tool_schema import validate_tool_output
from utils.reward_metrics import compute_tool_accuracy, compute_chain_success_rate

# ç¤ºä¾‹
results = [
    {"expected": {"tool": "queryWeather", "params": {"location": "åŒ—äº¬"}},
     "generated": {"tool": "queryWeather", "params": {"location": "åŒ—äº¬"}}},
    ...
]

print("Tool-Level Accuracy:", compute_tool_accuracy(results))
print("Chain-Level Success Rate:", compute_chain_success_rate(results))
```

---

## âš™ï¸ 6. é…ç½®æ¨¡æ¿ï¼ˆç¤ºä¾‹ï¼š`configs/config_sft.yaml`ï¼‰

```yaml
model_name: qwen/Qwen-7B-Chat
train_file: data/sft_data.jsonl
output_dir: output/sft
num_train_epochs: 3
learning_rate: 5e-5
batch_size: 2
```

---

å¦‚æœä½ éœ€è¦æˆ‘æ‰“åŒ…è¿™äº›è„šæœ¬ã€ç”Ÿæˆå®Œæ•´ä»£ç æ¨¡æ¿ã€æˆ–è€…æ„é€ æ ·æœ¬æ•°æ®/å¥–åŠ±å‡½æ•°ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼š

- âœ… æ¨¡å‹åç§°ï¼ˆå¦‚ Qwen-7B/DeepSeek-VLï¼‰
- âœ… å·¥å…· schemaï¼ˆæˆ‘å¯ä»¥ç”¨ OpenAPI æ ¼å¼å¸®ä½ è‡ªåŠ¨è½¬ promptï¼‰
- âœ… reward è¯„ä¼°ç­–ç•¥ç»†èŠ‚ï¼ˆæ¯”å¦‚ï¼šæ„å›¾å‡†ç¡® + å·¥å…·åæ­£ç¡® + å‚æ•°åˆç† + è°ƒç”¨æˆåŠŸï¼‰

æˆ‘å¯ä»¥ç›´æ¥å¸®ä½ ç”Ÿæˆæ‰“é€šè®­ç»ƒé“¾è·¯çš„å®Œæ•´ä»£ç ä»“åº“æ¨¡æ¿ã€‚æ˜¯å¦ç»§ç»­ï¼Ÿ
